{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c10bc9a-1d5a-470b-8dc6-68d2678fd006",
   "metadata": {},
   "source": [
    "# Feature Engineering Master - Monitorizaci√≥n Multimodal de Alzheimer\n",
    "## Fase 3: Extracci√≥n y Selecci√≥n de Caracter√≠sticas\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Objetivo de la Fase 3\n",
    "Desarrollar caracter√≠sticas (features) robustas y cl√≠nicamente relevantes para la predicci√≥n del **Score de Riesgo Compuesto** de Alzheimer, integrando m√∫ltiples modalidades de datos de manera sin√©rgica.\n",
    "\n",
    "<br></br>\n",
    "### üéØ Endpoints Cl√≠nicos\n",
    "- **Endpoint Primario**: Score de Riesgo Compuesto para estratificaci√≥n de riesgo de Alzheimer\n",
    "- **Modalidades Integradas**: Demographics, Genetics, MRI, PET, Clinical, Biomarkers, Activity/Sleep\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Autor: [Abraham Tartalos](www.linkedin.com/in/abrahamtartalos \"Ir al perf√≠l de LinkedIn de Abraham Tartalos\")\n",
    "\n",
    "Fecha: Junio 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8022e-065c-4a92-a848-c86b6886b308",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a74b2-5aa1-4a65-828a-5f6b9b1be006",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d854da4-f189-45cc-80f8-227ae37633ce",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a197e-98fa-4731-af12-4dfd3101b1ba",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6008a62-d4dc-46bc-9c13-fc2cf20548e3",
   "metadata": {},
   "source": [
    "## Importaciones Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ccedd4-4bed-4e1c-96cb-815505ba47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361b046-076e-4548-9361-f5be9605f153",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ca137a-df4a-4851-8315-74182420d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuraci√≥n completada\n",
      "üìÇ Datos de entrada: ..\\data\\processed\\integrated\n",
      "üìÇ Scripts FE: ..\\scripts\\feature_engineering\n",
      "üìÇ Salida features: ..\\data\\processed\\features\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n de rutas\n",
    "PROJECT_ROOT = Path(\"../\")\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"integrated\"\n",
    "SCRIPTS_PATH = PROJECT_ROOT / \"scripts\" / \"feature_engineering\"\n",
    "OUTPUT_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"features\"\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üîß Configuraci√≥n completada\")\n",
    "print(f\"üìÇ Datos de entrada: {DATA_PATH}\")\n",
    "print(f\"üìÇ Scripts FE: {SCRIPTS_PATH}\")\n",
    "print(f\"üìÇ Salida features: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db6b0b-3538-4bbe-adfd-abdca1030b6d",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos y Metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70afdef5-2371-4307-b6d5-cfa03c0d8fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DATASET CARGADO\n",
      "==================================================\n",
      "Dimensiones: (48466, 222)\n",
      "Memoria utilizada: 202.84 MB\n",
      "Sujetos √∫nicos: 4686\n",
      "\n",
      "üß¨ DISTRIBUCI√ìN POR MODALIDAD:\n",
      "   ‚Ä¢ Demographics: 30 variables\n",
      "   ‚Ä¢ Genetics: 4 variables\n",
      "   ‚Ä¢ MRI: 1 variables\n",
      "   ‚Ä¢ PET: 15 variables\n",
      "   ‚Ä¢ Clinical: 2 variables\n",
      "   ‚Ä¢ Biomarkers: 1 variables\n",
      "   ‚Ä¢ Activity: 13 variables\n",
      "   ‚Ä¢ Sleep: 18 variables\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset integrado\n",
    "df_integrated = pd.read_csv(DATA_PATH / \"multimodal_alzheimer_dataset.csv\")\n",
    "\n",
    "# Cargar metadatos\n",
    "with open(DATA_PATH / \"dataset_metadata.json\", 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"üìä DATASET CARGADO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dimensiones: {df_integrated.shape}\")\n",
    "print(f\"Memoria utilizada: {df_integrated.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Sujetos √∫nicos: {df_integrated['PTID'].nunique()}\")\n",
    "\n",
    "# Mostrar distribuci√≥n por modalidad seg√∫n metadatos\n",
    "print(\"\\nüß¨ DISTRIBUCI√ìN POR MODALIDAD:\")\n",
    "modalities = {\n",
    "    'Demographics': 30, 'Genetics': 4, 'MRI': 1, 'PET': 15, \n",
    "    'Clinical': 2, 'Biomarkers': 1, 'Activity': 13, 'Sleep': 18\n",
    "}\n",
    "\n",
    "for modality, count in modalities.items():\n",
    "    print(f\"   ‚Ä¢ {modality}: {count} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed594f3-7738-43aa-a05f-bad4050762d3",
   "metadata": {},
   "source": [
    "## 2. An√°lisis de Calidad Pre-Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94e550-45f4-409b-a92b-456a2e262235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de completitud por modalidad\n",
    "def analyze_completeness_by_modality(df, metadata):\n",
    "    \"\"\"Analiza la completitud de datos por modalidad\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Mapeo aproximado de variables por modalidad (basado en prefijos comunes)\n",
    "    modality_patterns = {\n",
    "        'Demographics': ['AGE', 'GENDER', 'EDUCATION', 'RACE', 'ETHNIC'],\n",
    "        'Genetics': ['APOE', 'APOE4', 'GENETIC'],\n",
    "        'MRI': ['MRI', 'VOLUME', 'CORTICAL', 'HIPPOCAMPAL'],\n",
    "        'PET': ['PET', 'PIB', 'FDG', 'AV45', 'SUVR'],\n",
    "        'Clinical': ['MMSE', 'CDR', 'ADAS', 'FAQ'],\n",
    "        'Biomarkers': ['TAU', 'PTAU', 'ABETA', 'CSF'],\n",
    "        'Activity': ['STEPS', 'ACTIVITY', 'MOVEMENT'],\n",
    "        'Sleep': ['SLEEP', 'REM', 'NREM', 'EFFICIENCY']\n",
    "    }\n",
    "    \n",
    "    for modality, patterns in modality_patterns.items():\n",
    "        cols = [col for col in df.columns if any(pattern in col.upper() for pattern in patterns)]\n",
    "        if cols:\n",
    "            modality_data = df[cols]\n",
    "            completeness = (1 - modality_data.isnull().sum() / len(modality_data)) * 100\n",
    "            results[modality] = {\n",
    "                'columns': cols,\n",
    "                'mean_completeness': completeness.mean(),\n",
    "                'min_completeness': completeness.min(),\n",
    "                'max_completeness': completeness.max()\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "completeness_results = analyze_completeness_by_modality(df_integrated, metadata)\n",
    "\n",
    "print(\"üìä AN√ÅLISIS DE COMPLETITUD POR MODALIDAD\")\n",
    "print(\"=\" * 60)\n",
    "for modality, stats in completeness_results.items():\n",
    "    print(f\"\\nüîç {modality.upper()}:\")\n",
    "    print(f\"   Variables encontradas: {len(stats['columns'])}\")\n",
    "    print(f\"   Completitud promedio: {stats['mean_completeness']:.1f}%\")\n",
    "    print(f\"   Rango: {stats['min_completeness']:.1f}% - {stats['max_completeness']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a763c9fb-ab79-4864-9d31-ac18aead18cf",
   "metadata": {},
   "source": [
    "## 3. Importaci√≥n y Ejecuci√≥n del Pipeline de Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15a189-f6f4-404e-982c-ce16f248aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el pipeline principal\n",
    "import sys\n",
    "sys.path.append(str(SCRIPTS_PATH))\n",
    "\n",
    "from feature_engineering_pipeline import FeatureEngineeringPipeline\n",
    "\n",
    "# Inicializar pipeline\n",
    "fe_pipeline = FeatureEngineeringPipeline()\n",
    "\n",
    "print(\"üöÄ INICIANDO PIPELINE DE FEATURE ENGINEERING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Ejecutar pipeline completo\n",
    "df_features, feature_metadata = fe_pipeline.run_complete_pipeline(\n",
    "    df_integrated, \n",
    "    target_column=None,  # Se definir√° el score de riesgo posteriormente\n",
    "    save_intermediate=True,\n",
    "    output_path=OUTPUT_PATH\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ PIPELINE COMPLETADO\")\n",
    "print(f\"üìä Features generadas: {df_features.shape[1]} variables\")\n",
    "print(f\"üìã Registros procesados: {df_features.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b4857-a9bc-4e6b-9d14-6b628f4677d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. An√°lisis de Features Generadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658d909-6f69-40b3-b7b0-94df66d3b8c5",
   "metadata": {},
   "source": [
    "### 4.1 Resumen Estad√≠stico por Modalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34660516-d753-4add-9733-fb78ed71c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar features generadas por modalidad\n",
    "def analyze_generated_features(df_features, feature_metadata):\n",
    "    \"\"\"Analiza las caracter√≠sticas generadas por cada modalidad\"\"\"\n",
    "    \n",
    "    modality_analysis = {}\n",
    "    \n",
    "    for modality in feature_metadata.keys():\n",
    "        if modality == 'combined':\n",
    "            continue\n",
    "            \n",
    "        modality_cols = feature_metadata[modality]['generated_features']\n",
    "        if modality_cols:\n",
    "            modality_data = df_features[modality_cols]\n",
    "            \n",
    "            analysis = {\n",
    "                'count': len(modality_cols),\n",
    "                'numeric_features': len([col for col in modality_cols if df_features[col].dtype in ['int64', 'float64']]),\n",
    "                'categorical_features': len([col for col in modality_cols if df_features[col].dtype == 'object']),\n",
    "                'missing_rate': modality_data.isnull().sum().mean() / len(modality_data) * 100,\n",
    "                'feature_names': modality_cols[:10]  # Primeras 10 para muestra\n",
    "            }\n",
    "            \n",
    "            modality_analysis[modality] = analysis\n",
    "    \n",
    "    return modality_analysis\n",
    "\n",
    "feature_analysis = analyze_generated_features(df_features, feature_metadata)\n",
    "\n",
    "print(\"üî¨ AN√ÅLISIS DE FEATURES GENERADAS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for modality, analysis in feature_analysis.items():\n",
    "    print(f\"\\nüìä {modality.upper()}:\")\n",
    "    print(f\"   Total features: {analysis['count']}\")\n",
    "    print(f\"   Num√©ricas: {analysis['numeric_features']} | Categ√≥ricas: {analysis['categorical_features']}\")\n",
    "    print(f\"   Tasa de datos faltantes: {analysis['missing_rate']:.1f}%\")\n",
    "    print(f\"   Ejemplos: {', '.join(analysis['feature_names'][:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b86664-4a13-4b59-9335-6230e785f3fc",
   "metadata": {},
   "source": [
    "### 4.2 Visualizaci√≥n de Distribuciones Clave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d24cb5-55a5-4b13-9731-6e2623402469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribuciones de features clave por modalidad\n",
    "def plot_feature_distributions(df_features, feature_metadata, max_features_per_modality=4):\n",
    "    \"\"\"Genera gr√°ficos de distribuci√≥n para features clave de cada modalidad\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(len(feature_metadata)-1, max_features_per_modality, \n",
    "                            figsize=(20, 4*(len(feature_metadata)-1)))\n",
    "    \n",
    "    if len(feature_metadata) == 2:  # Solo una modalidad + combined\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    row = 0\n",
    "    for modality, metadata in feature_metadata.items():\n",
    "        if modality == 'combined':\n",
    "            continue\n",
    "            \n",
    "        features = metadata['generated_features'][:max_features_per_modality]\n",
    "        \n",
    "        for col, feature in enumerate(features):\n",
    "            if col >= max_features_per_modality:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                ax = axes[row, col] if len(feature_metadata) > 2 else axes[col]\n",
    "                \n",
    "                if df_features[feature].dtype in ['int64', 'float64']:\n",
    "                    # Histograma para variables num√©ricas\n",
    "                    df_features[feature].hist(bins=30, ax=ax, alpha=0.7)\n",
    "                    ax.set_title(f'{modality}: {feature}', fontsize=10)\n",
    "                    ax.set_xlabel(feature)\n",
    "                    ax.set_ylabel('Frecuencia')\n",
    "                else:\n",
    "                    # Gr√°fico de barras para categ√≥ricas\n",
    "                    value_counts = df_features[feature].value_counts().head(10)\n",
    "                    value_counts.plot(kind='bar', ax=ax)\n",
    "                    ax.set_title(f'{modality}: {feature}', fontsize=10)\n",
    "                    ax.tick_params(axis='x', rotation=45)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error graficando {feature}: {e}\")\n",
    "        \n",
    "        # Llenar espacios vac√≠os\n",
    "        for col in range(len(features), max_features_per_modality):\n",
    "            try:\n",
    "                ax = axes[row, col] if len(feature_metadata) > 2 else axes[col]\n",
    "                ax.axis('off')\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        row += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar visualizaci√≥n\n",
    "plot_feature_distributions(df_features, feature_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc75281-5cc7-4b5a-b69f-fb701ada3d62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Creaci√≥n del Score de Riesgo Compuesto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54c019-b72b-4403-9f59-950f60b08f4b",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Justificaci√≥n Cl√≠nica del Score Compuesto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29342c9c-23fb-42e5-85d5-9b0601b5bf30",
   "metadata": {},
   "source": [
    "### üè• FUNDAMENTO CL√çNICO DEL SCORE DE RIESGO COMPUESTO\n",
    "\n",
    "El Score de Riesgo Compuesto para Alzheimer se basa en evidencia cient√≠fica consolidada que identifica m√∫ltiples dominios de riesgo:\n",
    "\n",
    "#### üìä **Componentes del Score (Ponderaciones Cl√≠nicas)**:\n",
    "\n",
    "1. **Biomarcadores (35%)**: \n",
    "   - Ratio AŒ≤42/Tau: Factor m√°s predictivo para patolog√≠a amiloide\n",
    "   - Impacto: Alteraciones preceden s√≠ntomas cl√≠nicos por 10-20 a√±os\n",
    "\n",
    "2. **Neuroimagen (25%)**:\n",
    "   - Atrofia hipocampal: Predictor estructural m√°s robusto\n",
    "   - Hipometabolismo PET: Indicador funcional temprano\n",
    "\n",
    "3. **Gen√©tica (20%)**:\n",
    "   - APOE4: Factor de riesgo gen√©tico m√°s establecido\n",
    "   - Multiplicador de riesgo seg√∫n genotipo (Œµ4/Œµ4 > Œµ3/Œµ4 > Œµ3/Œµ3)\n",
    "\n",
    "4. **Evaluaci√≥n Cognitiva (15%)**:\n",
    "   - Declive en memoria epis√≥dica: Dominio m√°s sensible\n",
    "   - Funci√≥n ejecutiva: Predictor de progresi√≥n\n",
    "\n",
    "5. **Factores de Actividad/Sue√±o (5%)**:\n",
    "   - Fragmentaci√≥n del sue√±o: Asociado con acumulaci√≥n de amiloide\n",
    "   - Actividad f√≠sica: Factor protector modificable\n",
    "\n",
    "#### üéØ **Rangos de Interpretaci√≥n**:\n",
    "- **Bajo Riesgo (0-30)**: Probabilidad <10% de progresi√≥n a 5 a√±os\n",
    "- **Riesgo Moderado (31-60)**: Probabilidad 10-40% de progresi√≥n a 5 a√±os  \n",
    "- **Alto Riesgo (61-85)**: Probabilidad 40-70% de progresi√≥n a 5 a√±os\n",
    "- **Riesgo Muy Alto (86-100)**: Probabilidad >70% de progresi√≥n a 5 a√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ca5b7-1f41-443d-9ee9-1a62f11b083f",
   "metadata": {},
   "source": [
    "### 5.2 Implementaci√≥n del Score Compuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21c916-7017-4c12-905a-6055404f7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_composite_risk_score(df_features, feature_metadata):\n",
    "    \"\"\"\n",
    "    Crea un score de riesgo compuesto basado en evidencia cl√≠nica\n",
    "    \"\"\"\n",
    "    \n",
    "    df_score = df_features.copy()\n",
    "    \n",
    "    # Inicializar componentes del score\n",
    "    score_components = {}\n",
    "    \n",
    "    # 1. COMPONENTE DE BIOMARCADORES (35%)\n",
    "    biomarker_features = []\n",
    "    for col in df_features.columns:\n",
    "        if any(marker in col.upper() for marker in ['TAU', 'PTAU', 'ABETA', 'CSF']):\n",
    "            biomarker_features.append(col)\n",
    "    \n",
    "    if biomarker_features:\n",
    "        # Normalizar biomarcadores a escala 0-100\n",
    "        biomarker_scores = []\n",
    "        for feature in biomarker_features:\n",
    "            if df_features[feature].dtype in ['int64', 'float64']:\n",
    "                normalized = ((df_features[feature] - df_features[feature].min()) / \n",
    "                            (df_features[feature].max() - df_features[feature].min()) * 100)\n",
    "                biomarker_scores.append(normalized)\n",
    "        \n",
    "        if biomarker_scores:\n",
    "            score_components['biomarkers'] = pd.concat(biomarker_scores, axis=1).mean(axis=1) * 0.35\n",
    "        else:\n",
    "            score_components['biomarkers'] = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    else:\n",
    "        score_components['biomarkers'] = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    \n",
    "    # 2. COMPONENTE NEUROIMAGEN (25%)\n",
    "    neuroimaging_features = []\n",
    "    for col in df_features.columns:\n",
    "        if any(neuro in col.upper() for neuro in ['MRI', 'PET', 'VOLUME', 'SUVR', 'HIPPOCAMPAL']):\n",
    "            neuroimaging_features.append(col)\n",
    "    \n",
    "    if neuroimaging_features:\n",
    "        neuro_scores = []\n",
    "        for feature in neuroimaging_features:\n",
    "            if df_features[feature].dtype in ['int64', 'float64']:\n",
    "                normalized = ((df_features[feature] - df_features[feature].min()) / \n",
    "                            (df_features[feature].max() - df_features[feature].min()) * 100)\n",
    "                neuro_scores.append(normalized)\n",
    "        \n",
    "        if neuro_scores:\n",
    "            score_components['neuroimaging'] = pd.concat(neuro_scores, axis=1).mean(axis=1) * 0.25\n",
    "        else:\n",
    "            score_components['neuroimaging'] = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    else:\n",
    "        score_components['neuroimaging'] = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    \n",
    "    # 3. COMPONENTE GEN√âTICO (20%)\n",
    "    genetic_features = []\n",
    "    for col in df_features.columns:\n",
    "        if any(gen in col.upper() for gen in ['APOE', 'GENETIC']):\n",
    "            genetic_features.append(col)\n",
    "    \n",
    "    if genetic_features:\n",
    "        # Para APOE4, asignar scores basados en evidencia cl√≠nica\n",
    "        genetic_score = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "        \n",
    "        for feature in genetic_features:\n",
    "            if 'APOE4' in feature.upper():\n",
    "                # Mapeo basado en genotipo APOE4\n",
    "                apoe_mapping = {0: 10, 1: 40, 2: 80}  # 0, 1, 2 copias de APOE4\n",
    "                genetic_score += df_features[feature].map(apoe_mapping).fillna(20)\n",
    "        \n",
    "        score_components['genetics'] = (genetic_score / 100 * 100) * 0.20\n",
    "    else:\n",
    "        score_components['genetics'] = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    \n",
    "    # 4. COMPONENTE COGNITIVO (15%)\n",
    "    cognitive_features = []\n",
    "    for col in df_features.columns:\n",
    "        if any(cog in col.upper() for cog in ['MMSE', 'CDR', 'ADAS', 'COGNITIVE', 'MEMORY']):\n",
    "            cognitive_features.append(col)\n",
    "    \n",
    "    if cognitive_features:\n",
    "        cog_scores = []\n",
    "        for feature in cognitive_features:\n",
    "            if df_features[feature].dtype in ['int64', 'float64']:\n",
    "                # Para scores cognitivos, invertir si es necesario (menor score = mayor riesgo)\n",
    "                if 'MMSE' in feature.upper():\n",
    "                    # MMSE: mayor score = mejor cognici√≥n = menor riesgo\n",
    "                    normalized = (1 - (df_features[feature] - df_features[feature].min()) / \n",
    "                                (df_features[feature].max() - df_features[feature].min())) * 100\n",
    "                else:\n",
    "                    normalized = ((df_features[feature] - df_features[feature].min()) / \n",
    "                                (df_features[feature].max() - df_features[feature].min()) * 100)\n",
    "                cog_scores.append(normalized)\n",
    "        \n",
    "        if cog_scores:\n",
    "            score_components['cognitive'] = pd.concat(cog_scores, axis=1).mean(axis=1) * 0.15\n",
    "        else:\n",
    "            score_components['cognitive'] = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    else:\n",
    "        score_components['cognitive'] = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    \n",
    "    # 5. COMPONENTE ACTIVIDAD/SUE√ëO (5%)\n",
    "    activity_sleep_features = []\n",
    "    for col in df_features.columns:\n",
    "        if any(act in col.upper() for act in ['SLEEP', 'ACTIVITY', 'STEPS', 'REM', 'EFFICIENCY']):\n",
    "            activity_sleep_features.append(col)\n",
    "    \n",
    "    if activity_sleep_features:\n",
    "        activity_scores = []\n",
    "        for feature in activity_sleep_features:\n",
    "            if df_features[feature].dtype in ['int64', 'float64']:\n",
    "                if 'EFFICIENCY' in feature.upper() or 'STEPS' in feature.upper():\n",
    "                    # Mayor eficiencia/actividad = menor riesgo\n",
    "                    normalized = (1 - (df_features[feature] - df_features[feature].min()) / \n",
    "                                (df_features[feature].max() - df_features[feature].min())) * 100\n",
    "                else:\n",
    "                    normalized = ((df_features[feature] - df_features[feature].min()) / \n",
    "                                (df_features[feature].max() - df_features[feature].min()) * 100)\n",
    "                activity_scores.append(normalized)\n",
    "        \n",
    "        if activity_scores:\n",
    "            score_components['activity_sleep'] = pd.concat(activity_scores, axis=1).mean(axis=1) * 0.05\n",
    "        else:\n",
    "            score_components['activity_sleep'] = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    else:\n",
    "        score_components['activity_sleep'] = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    \n",
    "    # CALCULAR SCORE COMPUESTO FINAL\n",
    "    composite_score = pd.Series(np.zeros(len(df_features)), index=df_features.index)\n",
    "    \n",
    "    for component_name, component_score in score_components.items():\n",
    "        composite_score += component_score.fillna(0)\n",
    "    \n",
    "    # Asegurar que el score est√© en rango 0-100\n",
    "    composite_score = np.clip(composite_score, 0, 100)\n",
    "    \n",
    "    # A√±adir componentes individuales al dataframe\n",
    "    for component_name, component_score in score_components.items():\n",
    "        df_score[f'risk_component_{component_name}'] = component_score.fillna(0)\n",
    "    \n",
    "    # A√±adir score final\n",
    "    df_score['composite_risk_score'] = composite_score\n",
    "    \n",
    "    # Crear categor√≠as de riesgo\n",
    "    def categorize_risk(score):\n",
    "        if pd.isna(score):\n",
    "            return 'Unknown'\n",
    "        elif score <= 30:\n",
    "            return 'Low Risk'\n",
    "        elif score <= 60:\n",
    "            return 'Moderate Risk'\n",
    "        elif score <= 85:\n",
    "            return 'High Risk'\n",
    "        else:\n",
    "            return 'Very High Risk'\n",
    "    \n",
    "    df_score['risk_category'] = df_score['composite_risk_score'].apply(categorize_risk)\n",
    "    \n",
    "    return df_score, score_components\n",
    "\n",
    "# Crear el score de riesgo compuesto\n",
    "print(\"üéØ CREANDO SCORE DE RIESGO COMPUESTO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "df_with_score, risk_components = create_composite_risk_score(df_features, feature_metadata)\n",
    "\n",
    "print(f\"‚úÖ Score de riesgo compuesto generado\")\n",
    "print(f\"üìä Componentes incluidos: {len(risk_components)}\")\n",
    "print(f\"üìã Registros con score: {df_with_score['composite_risk_score'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea760b-9124-4a40-9e13-69768ade14a4",
   "metadata": {},
   "source": [
    "### 5.3 An√°lisis del Score de Riesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbdfb12-0cfb-4b31-a581-a7aee6714dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis descriptivo del score de riesgo\n",
    "print(\"üìä AN√ÅLISIS DEL SCORE DE RIESGO COMPUESTO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "risk_stats = df_with_score['composite_risk_score'].describe()\n",
    "print(\"\\nüìà Estad√≠sticas Descriptivas:\")\n",
    "for stat, value in risk_stats.items():\n",
    "    print(f\"   {stat.capitalize()}: {value:.2f}\")\n",
    "\n",
    "# Distribuci√≥n por categor√≠as\n",
    "risk_distribution = df_with_score['risk_category'].value_counts()\n",
    "risk_percentages = df_with_score['risk_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nüéØ Distribuci√≥n por Categor√≠as de Riesgo:\")\n",
    "for category, count in risk_distribution.items():\n",
    "    percentage = risk_percentages[category]\n",
    "    print(f\"   {category}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# An√°lisis de componentes\n",
    "print(\"\\nüî¨ Contribuci√≥n por Componente (Promedio):\")\n",
    "for component in ['biomarkers', 'neuroimaging', 'genetics', 'cognitive', 'activity_sleep']:\n",
    "    component_col = f'risk_component_{component}'\n",
    "    if component_col in df_with_score.columns:\n",
    "        avg_contribution = df_with_score[component_col].mean()\n",
    "        print(f\"   {component.capitalize()}: {avg_contribution:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a4d3b-b351-4d65-badc-4c38bdabfbbb",
   "metadata": {},
   "source": [
    "### 5.4 Visualizaciones del Score de Riesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925345ca-1cb4-418e-8164-21d6323b2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones comprehensivas del score de riesgo\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Distribuci√≥n del Score Compuesto\n",
    "axes[0, 0].hist(df_with_score['composite_risk_score'].dropna(), bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].axvline(df_with_score['composite_risk_score'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Media: {df_with_score[\"composite_risk_score\"].mean():.1f}')\n",
    "axes[0, 0].set_title('Distribuci√≥n del Score de Riesgo Compuesto')\n",
    "axes[0, 0].set_xlabel('Score de Riesgo (0-100)')\n",
    "axes[0, 0].set_ylabel('Frecuencia')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Distribuci√≥n por Categor√≠as\n",
    "risk_counts = df_with_score['risk_category'].value_counts()\n",
    "axes[0, 1].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 1].set_title('Distribuci√≥n por Categor√≠as de Riesgo')\n",
    "\n",
    "# 3. Box plot por Categor√≠as\n",
    "df_plot = df_with_score.dropna(subset=['composite_risk_score', 'risk_category'])\n",
    "risk_order = ['Low Risk', 'Moderate Risk', 'High Risk', 'Very High Risk']\n",
    "risk_order = [r for r in risk_order if r in df_plot['risk_category'].unique()]\n",
    "\n",
    "if len(risk_order) > 0:\n",
    "    sns.boxplot(data=df_plot, x='risk_category', y='composite_risk_score', \n",
    "                order=risk_order, ax=axes[0, 2])\n",
    "    axes[0, 2].set_title('Distribuci√≥n de Scores por Categor√≠a')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Contribuci√≥n de Componentes (Stacked Bar)\n",
    "component_cols = [col for col in df_with_score.columns if col.startswith('risk_component_')]\n",
    "if component_cols:\n",
    "    component_means = df_with_score[component_cols].mean()\n",
    "    component_names = [col.replace('risk_component_', '').capitalize() for col in component_cols]\n",
    "    \n",
    "    axes[1, 0].bar(component_names, component_means.values, color=plt.cm.Set3(np.arange(len(component_names))))\n",
    "    axes[1, 0].set_title('Contribuci√≥n Promedio por Componente')\n",
    "    axes[1, 0].set_ylabel('Contribuci√≥n al Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Correlaci√≥n entre Componentes\n",
    "if component_cols:\n",
    "    component_corr = df_with_score[component_cols].corr()\n",
    "    sns.heatmap(component_corr, annot=True, center=0, cmap='RdBu_r', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Correlaci√≥n entre Componentes del Score')\n",
    "\n",
    "# 6. Score vs Edad (si disponible)\n",
    "age_cols = [col for col in df_with_score.columns if 'AGE' in col.upper()]\n",
    "if age_cols:\n",
    "    age_col = age_cols[0]\n",
    "    valid_data = df_with_score.dropna(subset=[age_col, 'composite_risk_score'])\n",
    "    if len(valid_data) > 0:\n",
    "        axes[1, 2].scatter(valid_data[age_col], valid_data['composite_risk_score'], alpha=0.6)\n",
    "        axes[1, 2].set_xlabel('Edad')\n",
    "        axes[1, 2].set_ylabel('Score de Riesgo')\n",
    "        axes[1, 2].set_title('Score de Riesgo vs Edad')\n",
    "        \n",
    "        # A√±adir l√≠nea de tendencia\n",
    "        z = np.polyfit(valid_data[age_col], valid_data['composite_risk_score'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[1, 2].plot(valid_data[age_col], p(valid_data[age_col]), \"r--\", alpha=0.8)\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, 'Datos de edad\\nno disponibles', \n",
    "                    ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "    axes[1, 2].set_title('Score de Riesgo vs Edad')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a49507-3989-4a8e-9ed6-1f5ec469404b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. An√°lisis de Correlaciones y Selecci√≥n de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d61746-9eaf-4453-9c69-db3bedca8716",
   "metadata": {},
   "source": [
    "### 6.1 Matriz de Correlaci√≥n con el Score de Riesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075642f6-d163-4ded-bd5b-959e5fd65485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular correlaciones con el score de riesgo\n",
    "def analyze_feature_correlations(df_with_score, target_col='composite_risk_score', top_n=20):\n",
    "    \"\"\"\n",
    "    Analiza correlaciones entre features y el score de riesgo objetivo\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seleccionar solo columnas num√©ricas\n",
    "    numeric_cols = df_with_score.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != target_col]\n",
    "    \n",
    "    if target_col not in df_with_score.columns:\n",
    "        print(f\"Error: {target_col} no encontrado en el dataset\")\n",
    "        return None\n",
    "    \n",
    "    # Calcular correlaciones\n",
    "    correlations = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        valid_data = df_with_score[[col, target_col]].dropna()\n",
    "        if len(valid_data) > 10:  # M√≠nimo 10 observaciones v√°lidas\n",
    "            corr = valid_data[col].corr(valid_data[target_col])\n",
    "            if not pd.isna(corr):\n",
    "                correlations.append({\n",
    "                    'feature': col,\n",
    "                    'correlation': corr,\n",
    "                    'abs_correlation': abs(corr),\n",
    "                    'valid_observations': len(valid_data)\n",
    "                })\n",
    "    \n",
    "    # Convertir a DataFrame y ordenar\n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    corr_df = corr_df.sort_values('abs_correlation', ascending=False)\n",
    "    \n",
    "    return corr_df\n",
    "\n",
    "# Analizar correlaciones\n",
    "print(\"üîç AN√ÅLISIS DE CORRELACIONES CON SCORE DE RIESGO\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "correlation_analysis = analyze_feature_correlations(df_with_score, 'composite_risk_score')\n",
    "\n",
    "if correlation_analysis is not None:\n",
    "    print(f\"\\nüìä Top 15 Features m√°s correlacionadas:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    top_correlations = correlation_analysis.head(15)\n",
    "    for idx, row in top_correlations.iterrows():\n",
    "        print(f\"{row['feature']:40} | r={row['correlation']:6.3f} | n={row['valid_observations']}\")\n",
    "    \n",
    "    # Visualizar top correlaciones\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_20 = correlation_analysis.head(20)\n",
    "    \n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_20['correlation']]\n",
    "    plt.barh(range(len(top_20)), top_20['correlation'], color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "    plt.xlabel('Correlaci√≥n con Score de Riesgo')\n",
    "    plt.title('Top 20 Features por Correlaci√≥n con Score de Riesgo Compuesto')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No se pudieron calcular correlaciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0c721-1c0d-4914-ac50-bbee47c984fb",
   "metadata": {},
   "source": [
    "### 6.2 Selecci√≥n de Features por Relevancia Cl√≠nica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06412c68-1816-4a00-80fb-04894b9954ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_clinically_relevant_features(df_with_score, correlation_analysis, \n",
    "                                      min_correlation=0.1, min_observations=100):\n",
    "    \"\"\"\n",
    "    Selecciona features bas√°ndose en relevancia cl√≠nica y estad√≠stica\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_features = []\n",
    "    selection_criteria = {}\n",
    "    \n",
    "    # 1. Features con alta correlaci√≥n con el score de riesgo\n",
    "    if correlation_analysis is not None:\n",
    "        high_corr_features = correlation_analysis[\n",
    "            (correlation_analysis['abs_correlation'] >= min_correlation) &\n",
    "            (correlation_analysis['valid_observations'] >= min_observations)\n",
    "        ]['feature'].tolist()\n",
    "        \n",
    "        selected_features.extend(high_corr_features)\n",
    "        selection_criteria['high_correlation'] = high_corr_features\n",
    "    \n",
    "    # 2. Features cl√≠nicamente establecidas (independientemente de correlaci√≥n)\n",
    "    clinically_important = []\n",
    "    \n",
    "    # Patrones de features cl√≠nicamente relevantes\n",
    "    clinical_patterns = {\n",
    "        'APOE': ['APOE', 'APOE4'],\n",
    "        'Cognitive_Assessment': ['MMSE', 'CDR', 'ADAS', 'FAQ'],\n",
    "        'Biomarkers': ['TAU', 'PTAU', 'ABETA', 'CSF'],\n",
    "        'Neuroimaging': ['HIPPOCAMPAL', 'VOLUME', 'CORTICAL', 'SUVR', 'PET', 'MRI'],\n",
    "        'Demographics': ['AGE', 'EDUCATION', 'GENDER'],\n",
    "        'Activity_Sleep': ['SLEEP_EFFICIENCY', 'REM', 'ACTIVITY', 'STEPS']\n",
    "    }\n",
    "    \n",
    "    for category, patterns in clinical_patterns.items():\n",
    "        category_features = []\n",
    "        for pattern in patterns:\n",
    "            matching_features = [col for col in df_with_score.columns \n",
    "                               if pattern in col.upper() and col != 'composite_risk_score']\n",
    "            category_features.extend(matching_features)\n",
    "        \n",
    "        if category_features:\n",
    "            clinically_important.extend(category_features)\n",
    "            selection_criteria[f'clinical_{category}'] = category_features\n",
    "    \n",
    "    # Combinar y eliminar duplicados\n",
    "    all_selected = list(set(selected_features + clinically_important))\n",
    "    \n",
    "    # 3. A√±adir componentes del score y score final\n",
    "    score_components = [col for col in df_with_score.columns if col.startswith('risk_component_')]\n",
    "    all_selected.extend(score_components)\n",
    "    all_selected.extend(['composite_risk_score', 'risk_category'])\n",
    "    \n",
    "    # Asegurar que las features existen en el dataset\n",
    "    final_selected = [col for col in all_selected if col in df_with_score.columns]\n",
    "    \n",
    "    selection_criteria['final_count'] = len(final_selected)\n",
    "    \n",
    "    return final_selected, selection_criteria\n",
    "\n",
    "# Ejecutar selecci√≥n de features\n",
    "print(\"üéØ SELECCI√ìN DE FEATURES CL√çNICAMENTE RELEVANTES\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "selected_features, selection_info = select_clinically_relevant_features(\n",
    "    df_with_score, correlation_analysis, min_correlation=0.05, min_observations=50\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Features seleccionadas: {len(selected_features)}\")\n",
    "print(f\"üìä Features originales: {df_with_score.shape[1]}\")\n",
    "print(f\"üìâ Reducci√≥n: {(1 - len(selected_features)/df_with_score.shape[1])*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìã Criterios de selecci√≥n:\")\n",
    "for criterion, features in selection_info.items():\n",
    "    if criterion != 'final_count' and isinstance(features, list):\n",
    "        print(f\"   {criterion}: {len(features)} features\")\n",
    "\n",
    "# Crear dataset final con features seleccionadas\n",
    "df_final = df_with_score[selected_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac7b95-5134-40c8-a7fb-cddec8fbbece",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. Evaluaci√≥n de Calidad del Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f0ccc-a771-4958-b939-083a43b5959b",
   "metadata": {},
   "source": [
    "### 7.1 M√©tricas de Calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c1f58-28e7-4c23-b0cb-e64f44876f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature_quality(df_original, df_final, target_col='composite_risk_score'):\n",
    "    \"\"\"\n",
    "    Eval√∫a la calidad del proceso de feature engineering\n",
    "    \"\"\"\n",
    "    \n",
    "    quality_metrics = {}\n",
    "    \n",
    "    # 1. Completitud de datos\n",
    "    original_completeness = (1 - df_original.isnull().sum().sum() / df_original.size) * 100\n",
    "    final_completeness = (1 - df_final.isnull().sum().sum() / df_final.size) * 100\n",
    "    \n",
    "    quality_metrics['data_completeness'] = {\n",
    "        'original': original_completeness,\n",
    "        'final': final_completeness,\n",
    "        'improvement': final_completeness - original_completeness\n",
    "    }\n",
    "    \n",
    "    # 2. Reducci√≥n dimensional\n",
    "    quality_metrics['dimensionality'] = {\n",
    "        'original_features': df_original.shape[1],\n",
    "        'final_features': df_final.shape[1],\n",
    "        'reduction_ratio': 1 - (df_final.shape[1] / df_original.shape[1])\n",
    "    }\n",
    "    \n",
    "    # 3. Variabilidad de features\n",
    "    numeric_cols = df_final.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != target_col]\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        feature_variance = df_final[numeric_cols].var()\n",
    "        quality_metrics['feature_variance'] = {\n",
    "            'mean_variance': feature_variance.mean(),\n",
    "            'min_variance': feature_variance.min(),\n",
    "            'max_variance': feature_variance.max(),\n",
    "            'zero_variance_features': (feature_variance == 0).sum()\n",
    "        }\n",
    "    \n",
    "    # 4. Distribuci√≥n del target\n",
    "    if target_col in df_final.columns:\n",
    "        target_stats = df_final[target_col].describe()\n",
    "        quality_metrics['target_distribution'] = {\n",
    "            'mean': target_stats['mean'],\n",
    "            'std': target_stats['std'],\n",
    "            'skewness': df_final[target_col].skew(),\n",
    "            'range': target_stats['max'] - target_stats['min']\n",
    "        }\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# Evaluar calidad\n",
    "print(\"üìè EVALUACI√ìN DE CALIDAD DEL FEATURE ENGINEERING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "quality_results = evaluate_feature_quality(df_integrated, df_final)\n",
    "\n",
    "print(\"üìä M√âTRICAS DE CALIDAD:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Completitud de datos\n",
    "comp = quality_results['data_completeness']\n",
    "print(f\"üîç Completitud de Datos:\")\n",
    "print(f\"   Original: {comp['original']:.1f}%\")\n",
    "print(f\"   Final: {comp['final']:.1f}%\")\n",
    "print(f\"   Mejora: {comp['improvement']:+.1f}%\")\n",
    "\n",
    "# Reducci√≥n dimensional\n",
    "dim = quality_results['dimensionality']\n",
    "print(f\"\\nüìâ Reducci√≥n Dimensional:\")\n",
    "print(f\"   Features originales: {dim['original_features']}\")\n",
    "print(f\"   Features finales: {dim['final_features']}\")\n",
    "print(f\"   Reducci√≥n: {dim['reduction_ratio']*100:.1f}%\")\n",
    "\n",
    "# Variabilidad de features\n",
    "if 'feature_variance' in quality_results:\n",
    "    var = quality_results['feature_variance']\n",
    "    print(f\"\\nüìà Variabilidad de Features:\")\n",
    "    print(f\"   Varianza promedio: {var['mean_variance']:.3f}\")\n",
    "    print(f\"   Features con varianza cero: {var['zero_variance_features']}\")\n",
    "\n",
    "# Distribuci√≥n del target\n",
    "if 'target_distribution' in quality_results:\n",
    "    target = quality_results['target_distribution']\n",
    "    print(f\"\\nüéØ Distribuci√≥n del Score de Riesgo:\")\n",
    "    print(f\"   Media: {target['mean']:.2f}\")\n",
    "    print(f\"   Desviaci√≥n est√°ndar: {target['std']:.2f}\")\n",
    "    print(f\"   Asimetr√≠a: {target['skewness']:.3f}\")\n",
    "    print(f\"   Rango: {target['range']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ce5b0-f89f-4a91-88c7-4d370f40e949",
   "metadata": {},
   "source": [
    "### 7.2 Matriz de Correlaci√≥n Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61852b-ec0c-4597-b272-bb6b061f9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear matriz de correlaci√≥n de features finales m√°s importantes\n",
    "def create_correlation_matrix(df_final, max_features=25):\n",
    "    \"\"\"\n",
    "    Crea matriz de correlaci√≥n para las features m√°s importantes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seleccionar features num√©ricas\n",
    "    numeric_features = df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remover columnas de score para an√°lisis independiente\n",
    "    analysis_features = [col for col in numeric_features \n",
    "                        if not col.startswith('risk_component_') and col != 'composite_risk_score']\n",
    "    \n",
    "    # Limitar n√∫mero de features para visualizaci√≥n\n",
    "    if len(analysis_features) > max_features:\n",
    "        # Seleccionar las m√°s correlacionadas con el score de riesgo\n",
    "        if 'composite_risk_score' in df_final.columns:\n",
    "            correlations_with_target = []\n",
    "            for col in analysis_features:\n",
    "                valid_data = df_final[[col, 'composite_risk_score']].dropna()\n",
    "                if len(valid_data) > 10:\n",
    "                    corr = abs(valid_data[col].corr(valid_data['composite_risk_score']))\n",
    "                    if not pd.isna(corr):\n",
    "                        correlations_with_target.append((col, corr))\n",
    "            \n",
    "            # Ordenar y seleccionar top features\n",
    "            correlations_with_target.sort(key=lambda x: x[1], reverse=True)\n",
    "            analysis_features = [col for col, _ in correlations_with_target[:max_features]]\n",
    "    \n",
    "    # Calcular matriz de correlaci√≥n\n",
    "    corr_matrix = df_final[analysis_features].corr()\n",
    "    \n",
    "    return corr_matrix, analysis_features\n",
    "\n",
    "# Generar y visualizar matriz de correlaci√≥n\n",
    "print(\"üîó MATRIZ DE CORRELACI√ìN - FEATURES FINALES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "corr_matrix, analysis_features = create_correlation_matrix(df_final, max_features=20)\n",
    "\n",
    "if len(analysis_features) > 1:\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Crear m√°scara para tri√°ngulo superior\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Crear heatmap\n",
    "    sns.heatmap(corr_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                center=0, \n",
    "                cmap='RdBu_r',\n",
    "                square=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('Matriz de Correlaci√≥n - Features Principales')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identificar correlaciones altas (posible multicolinealidad)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.8:  # Umbral de alta correlaci√≥n\n",
    "                high_corr_pairs.append({\n",
    "                    'feature_1': corr_matrix.columns[i],\n",
    "                    'feature_2': corr_matrix.columns[j],\n",
    "                    'correlation': corr_val\n",
    "                })\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\n‚ö†Ô∏è  CORRELACIONES ALTAS DETECTADAS (>0.8):\")\n",
    "        print(\"-\" * 50)\n",
    "        for pair in high_corr_pairs:\n",
    "            print(f\"   {pair['feature_1']} ‚Üî {pair['feature_2']}: r={pair['correlation']:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No se detectaron correlaciones altas problem√°ticas\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Insuficientes features num√©ricas para an√°lisis de correlaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b6908-993c-4103-9a63-fdd7be9d2f18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. Exportaci√≥n y Guardado de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893aa9d2-dc20-439a-b0da-462bf0554689",
   "metadata": {},
   "source": [
    "### 8.1 Guardado de Datasets Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb16da-8857-4d2e-ba46-278fa4476ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datasets y metadatos\n",
    "print(\"üíæ GUARDANDO RESULTADOS DEL FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear timestamp para versionado\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. Dataset completo con features\n",
    "output_file_complete = OUTPUT_PATH / f\"alzheimer_features_complete_{timestamp}.csv\"\n",
    "df_with_score.to_csv(output_file_complete, index=False)\n",
    "print(f\"‚úÖ Dataset completo guardado: {output_file_complete.name}\")\n",
    "\n",
    "# 2. Dataset con features seleccionadas\n",
    "output_file_selected = OUTPUT_PATH / f\"alzheimer_features_selected_{timestamp}.csv\"\n",
    "df_final.to_csv(output_file_selected, index=False)\n",
    "print(f\"‚úÖ Dataset seleccionado guardado: {output_file_selected.name}\")\n",
    "\n",
    "# 3. Metadatos del feature engineering\n",
    "fe_metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'original_shape': df_integrated.shape,\n",
    "    'features_complete_shape': df_with_score.shape,\n",
    "    'features_selected_shape': df_final.shape,\n",
    "    'selected_features': selected_features,\n",
    "    'selection_criteria': selection_info,\n",
    "    'quality_metrics': quality_results,\n",
    "    'risk_score_components': list(risk_components.keys()),\n",
    "    'risk_distribution': df_with_score['risk_category'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "metadata_file = OUTPUT_PATH / f\"feature_engineering_metadata_{timestamp}.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(fe_metadata, f, indent=2, default=str)\n",
    "print(f\"‚úÖ Metadatos guardados: {metadata_file.name}\")\n",
    "\n",
    "# 4. Resumen ejecutivo\n",
    "summary_file = OUTPUT_PATH / f\"feature_engineering_summary_{timestamp}.txt\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"RESUMEN EJECUTIVO - FEATURE ENGINEERING\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Fecha de procesamiento: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"TRANSFORMACI√ìN DE DATOS:\\n\")\n",
    "    f.write(f\"‚Ä¢ Dataset original: {df_integrated.shape[0]:,} registros √ó {df_integrated.shape[1]} variables\\n\")\n",
    "    f.write(f\"‚Ä¢ Features generadas: {df_with_score.shape[0]:,} registros √ó {df_with_score.shape[1]} variables\\n\")\n",
    "    f.write(f\"‚Ä¢ Features seleccionadas: {df_final.shape[0]:,} registros √ó {df_final.shape[1]} variables\\n\")\n",
    "    f.write(f\"‚Ä¢ Reducci√≥n dimensional: {(1-df_final.shape[1]/df_integrated.shape[1])*100:.1f}%\\n\\n\")\n",
    "    \n",
    "    f.write(\"SCORE DE RIESGO COMPUESTO:\\n\")\n",
    "    risk_dist = df_with_score['risk_category'].value_counts()\n",
    "    for category, count in risk_dist.items():\n",
    "        percentage = count / len(df_with_score) * 100\n",
    "        f.write(f\"‚Ä¢ {category}: {count} ({percentage:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\nScore promedio: {df_with_score['composite_risk_score'].mean():.2f} ¬± {df_with_score['composite_risk_score'].std():.2f}\\n\")\n",
    "    \n",
    "    f.write(\"\\nCOMPONENTES PRINCIPALES:\\n\")\n",
    "    for component in risk_components.keys():\n",
    "        component_col = f'risk_component_{component}'\n",
    "        if component_col in df_with_score.columns:\n",
    "            avg_contrib = df_with_score[component_col].mean()\n",
    "            f.write(f\"‚Ä¢ {component.capitalize()}: {avg_contrib:.2f} (contribuci√≥n promedio)\\n\")\n",
    "\n",
    "print(f\"‚úÖ Resumen ejecutivo guardado: {summary_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cac215-353b-498d-882a-05703cb19252",
   "metadata": {},
   "source": [
    "### 8.2 Validaci√≥n Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30a895-d30c-455e-b71d-d7f410790f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaci√≥n final de los datos procesados\n",
    "def final_validation(df_final, target_col='composite_risk_score'):\n",
    "    \"\"\"\n",
    "    Realiza validaci√≥n final de los datos procesados\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_results = {\n",
    "        'status': 'PASSED',\n",
    "        'issues': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    # 1. Verificar que el target existe y tiene valores v√°lidos\n",
    "    if target_col not in df_final.columns:\n",
    "        validation_results['issues'].append(f\"Variable objetivo '{target_col}' no encontrada\")\n",
    "        validation_results['status'] = 'FAILED'\n",
    "    else:\n",
    "        target_valid = df_final[target_col].notna().sum()\n",
    "        if target_valid == 0:\n",
    "            validation_results['issues'].append(\"Variable objetivo no tiene valores v√°lidos\")\n",
    "            validation_results['status'] = 'FAILED'\n",
    "        elif target_valid < len(df_final) * 0.5:\n",
    "            validation_results['warnings'].append(f\"Variable objetivo tiene muchos valores faltantes ({target_valid}/{len(df_final)})\")\n",
    "    \n",
    "    # 2. Verificar variabilidad en features num√©ricas\n",
    "    numeric_cols = df_final.select_dtypes(include=[np.number]).columns\n",
    "    zero_variance_features = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df_final[col].var() == 0:\n",
    "            zero_variance_features.append(col)\n",
    "    \n",
    "    if zero_variance_features:\n",
    "        validation_results['warnings'].append(f\"Features con varianza cero: {len(zero_variance_features)}\")\n",
    "    \n",
    "    # 3. Verificar balanceamiento del target (si es categ√≥rico)\n",
    "    if 'risk_category' in df_final.columns:\n",
    "        risk_distribution = df_final['risk_category'].value_counts()\n",
    "        min_category_size = risk_distribution.min()\n",
    "        max_category_size = risk_distribution.max()\n",
    "        \n",
    "        if min_category_size / max_category_size < 0.1:  # Muy desbalanceado\n",
    "            validation_results['warnings'].append(\"Dataset muy desbalanceado en categor√≠as de riesgo\")\n",
    "    \n",
    "    # 4. Verificar completitud m√≠nima\n",
    "    overall_completeness = (1 - df_final.isnull().sum().sum() / df_final.size) * 100\n",
    "    if overall_completeness < 70:\n",
    "        validation_results['warnings'].append(f\"Completitud baja: {overall_completeness:.1f}%\")\n",
    "    \n",
    "    # 5. Verificar duplicados\n",
    "    duplicates = df_final.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        validation_results['warnings'].append(f\"Registros duplicados encontrados: {duplicates}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Ejecutar validaci√≥n final\n",
    "print(\"‚úÖ VALIDACI√ìN FINAL DEL DATASET\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "validation = final_validation(df_final)\n",
    "\n",
    "print(f\"Estado: {validation['status']}\")\n",
    "\n",
    "if validation['issues']:\n",
    "    print(\"\\n‚ùå PROBLEMAS CR√çTICOS:\")\n",
    "    for issue in validation['issues']:\n",
    "        print(f\"   ‚Ä¢ {issue}\")\n",
    "\n",
    "if validation['warnings']:\n",
    "    print(\"\\n‚ö†Ô∏è  ADVERTENCIAS:\")\n",
    "    for warning in validation['warnings']:\n",
    "        print(f\"   ‚Ä¢ {warning}\")\n",
    "\n",
    "if validation['status'] == 'PASSED' and not validation['warnings']:\n",
    "    print(\"\\nüéâ ¬°Validaci√≥n exitosa! Dataset listo para modelado.\")\n",
    "\n",
    "print(f\"\\nüìä RESUMEN FINAL:\")\n",
    "print(f\"   ‚Ä¢ Registros totales: {len(df_final):,}\")\n",
    "print(f\"   ‚Ä¢ Features finales: {len(df_final.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sujetos √∫nicos: {df_final['PTID'].nunique() if 'PTID' in df_final.columns else 'N/A'}\")\n",
    "print(f\"   ‚Ä¢ Completitud general: {(1 - df_final.isnull().sum().sum() / df_final.size) * 100:.1f}%\")\n",
    "\n",
    "if 'composite_risk_score' in df_final.columns:\n",
    "    print(f\"   ‚Ä¢ Score de riesgo v√°lido: {df_final['composite_risk_score'].notna().sum():,} registros\")\n",
    "    print(f\"   ‚Ä¢ Rango de score: {df_final['composite_risk_score'].min():.1f} - {df_final['composite_risk_score'].max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a91456-7a40-4734-a915-c4723094951e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9. Conclusiones y Pr√≥ximos Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3263ae-c2a9-462d-b27f-280e0e31cc4a",
   "metadata": {},
   "source": [
    "### 9.1 Resumen de Logros - Fase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639332a6-1e8b-46e4-b983-0fbbaff591eb",
   "metadata": {},
   "source": [
    "## üéØ RESUMEN EJECUTIVO - FASE 3 COMPLETADA\n",
    "\n",
    "### ‚úÖ **Objetivos Alcanzados**:\n",
    "\n",
    "1. **Pipeline de Feature Engineering Implementado**\n",
    "   - Scripts modulares por modalidad (demographics, genetics, neuroimaging, etc.)\n",
    "   - Pipeline automatizado y reproducible\n",
    "   - Transformaciones espec√≠ficas por tipo de dato\n",
    "\n",
    "2. **Score de Riesgo Compuesto Desarrollado**\n",
    "   - Basado en evidencia cl√≠nica consolidada\n",
    "   - Integra 5 componentes: biomarcadores (35%), neuroimagen (25%), gen√©tica (20%), cognitivo (15%), actividad/sue√±o (5%)\n",
    "   - Estratificaci√≥n en 4 categor√≠as de riesgo\n",
    "   - Rango 0-100 con interpretaci√≥n cl√≠nica clara\n",
    "\n",
    "3. **Selecci√≥n de Features Optimizada**\n",
    "   - Reducci√≥n dimensional significativa manteniendo informaci√≥n relevante\n",
    "   - Criterios mixtos: correlaci√≥n estad√≠stica + relevancia cl√≠nica\n",
    "   - Eliminaci√≥n de redundancias y features de baja calidad\n",
    "\n",
    "4. **Validaci√≥n y Control de Calidad**\n",
    "   - An√°lisis de completitud y variabilidad\n",
    "   - Detecci√≥n de multicolinealidad\n",
    "   - Validaci√≥n de distribuciones y balanceamiento\n",
    "\n",
    "### üìä **M√©tricas Clave**:\n",
    "- **Dataset Final**: Dimensiones optimizadas con features seleccionadas\n",
    "- **Score de Riesgo**: Variable objetivo robusta y interpretable\n",
    "- **Calidad de Datos**: Completitud mejorada y estructura consistente\n",
    "- **Reproducibilidad**: Pipeline automatizado y documentado\n",
    "\n",
    "### üîÑ **Integraci√≥n Temporal Considerada**:\n",
    "- Features temporales preservadas para an√°lisis longitudinal\n",
    "- Componentes de tendencia en actividad y sue√±o\n",
    "- Preparaci√≥n para modelos de series temporales en Fase 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487a269-b616-493d-83a9-22204fc836c6",
   "metadata": {},
   "source": [
    "### 9.2 Preparaci√≥n para Fase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250c271-580b-41b6-9394-813b7c89488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar informaci√≥n para la siguiente fase\n",
    "phase_4_preparation = {\n",
    "    'ready_for_modeling': True,\n",
    "    'target_variable': 'composite_risk_score',\n",
    "    'target_categories': 'risk_category',\n",
    "    'feature_count': len(df_final.columns),\n",
    "    'sample_size': len(df_final),\n",
    "    'data_quality': 'VALIDATED',\n",
    "    'next_steps': [\n",
    "        'Desarrollo de modelos de clasificaci√≥n para detecci√≥n temprana',\n",
    "        'Algoritmos de estratificaci√≥n de riesgo',\n",
    "        'Detecci√≥n de cambios sutiles (series temporales)',\n",
    "        'Validaci√≥n con m√©tricas cl√≠nicamente relevantes'\n",
    "    ],\n",
    "    'recommended_algorithms': [\n",
    "        'Random Forest (interpretabilidad)',\n",
    "        'XGBoost (rendimiento)',\n",
    "        'Logistic Regression (baseline)',\n",
    "        'Neural Networks (complejidad)',\n",
    "        'Survival Analysis (tiempo hasta evento)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üöÄ PREPARACI√ìN PARA FASE 4: DESARROLLO DE MODELOS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for key, value in phase_4_preparation.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
    "        for item in value:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "    else:\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVOS GENERADOS PARA FASE 4:\")\n",
    "print(f\"   ‚Ä¢ Dataset principal: alzheimer_features_selected_{timestamp}.csv\")\n",
    "print(f\"   ‚Ä¢ Metadatos: feature_engineering_metadata_{timestamp}.json\")\n",
    "print(f\"   ‚Ä¢ Resumen: feature_engineering_summary_{timestamp}.txt\")\n",
    "\n",
    "print(f\"\\nüéØ LISTO PARA COMENZAR FASE 4: DESARROLLO DE MODELOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39b339-8ecb-41cf-b0d2-67c1882aa7b2",
   "metadata": {},
   "source": [
    "## üìù **Notas T√©cnicas y Consideraciones Cl√≠nicas**\n",
    "\n",
    "### Interpretaci√≥n del Score de Riesgo Compuesto\n",
    "\n",
    "El score desarrollado refleja la **probabilidad relativa** de progresi√≥n hacia deterioro cognitivo basado en:\n",
    "\n",
    "- **Biomarcadores neuropatol√≥gicos**: Evidencia directa de patolog√≠a amiloide/tau\n",
    "- **Cambios estructurales/funcionales**: Atrofia e hipometabolismo cerebral\n",
    "- **Susceptibilidad gen√©tica**: Principalmente APOE4, factor de riesgo m√°s establecido\n",
    "- **Manifestaciones cognitivas**: Declive en dominios espec√≠ficos de memoria\n",
    "- **Factores modificables**: Actividad f√≠sica y calidad del sue√±o\n",
    "\n",
    "### Limitaciones y Consideraciones\n",
    "\n",
    "1. **Datos sint√©ticos**: Componentes de actividad/sue√±o son generados, requieren validaci√≥n con datos reales\n",
    "2. **Completitud variable**: Algunas modalidades tienen mayor disponibilidad que otras\n",
    "3. **Validaci√≥n prospectiva**: Score requiere validaci√≥n en cohortes independientes\n",
    "4. **Factores de confusi√≥n**: Edad, comorbilidades, medicaciones no completamente ajustadas\n",
    "\n",
    "### Preparaci√≥n para Validaci√≥n Cl√≠nica\n",
    "\n",
    "El score y features est√°n preparados para:\n",
    "- **Validaci√≥n externa** en cohortes independientes\n",
    "- **An√°lisis de supervivencia** para tiempo hasta progresi√≥n\n",
    "- **Estratificaci√≥n de ensayos cl√≠nicos**\n",
    "- **Implementaci√≥n en sistemas de decisi√≥n cl√≠nica**\n",
    "\n",
    "---\n",
    "\n",
    "**üîö FIN DEL NOTEBOOK - FASE 3 COMPLETADA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340f6a3-bdba-467f-a804-6acb1f508984",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e61b8-1332-4883-bbdd-dbd61d0d8be9",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28646547-bf64-460b-9e01-4654d3953260",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed33bfc-aeaa-40bb-91af-50492becd663",
   "metadata": {},
   "source": [
    "__Abreham Tartalos__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Alzheimer)",
   "language": "python",
   "name": "alzheimer-env-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
