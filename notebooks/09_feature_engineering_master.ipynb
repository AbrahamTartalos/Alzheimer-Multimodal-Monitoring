{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c10bc9a-1d5a-470b-8dc6-68d2678fd006",
   "metadata": {},
   "source": [
    "Feature Engineering Pipeline Master\n",
    "====================================\n",
    "Sistema principal para el procesamiento de features multimodales para Alzheimer\n",
    "\n",
    "Autor: [Abraham Tartalos](www.linkedin.com/in/abrahamtartalos \"Ir al perf√≠l de LinkedIn de Abraham Tartalos\")\n",
    "\n",
    "Fecha: Mayo 2025\n",
    "\n",
    "Fase: 3 - Feature Engineering y Selecci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8022e-065c-4a92-a848-c86b6886b308",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a74b2-5aa1-4a65-828a-5f6b9b1be006",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d854da4-f189-45cc-80f8-227ae37633ce",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a197e-98fa-4731-af12-4dfd3101b1ba",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6008a62-d4dc-46bc-9c13-fc2cf20548e3",
   "metadata": {},
   "source": [
    "## Importar Librer√≠as Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccedd4-4bed-4e1c-96cb-815505ba47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Importar m√≥dulos espec√≠ficos de feature engineering\n",
    "from fe_demographics import DemographicsFeatureEngineering\n",
    "from fe_genetics import GeneticsFeatureEngineering\n",
    "from fe_neuroimaging import NeuroimagingFeatureEngineering\n",
    "from fe_biomarkers import BiomarkersFeatureEngineering\n",
    "from fe_clinical import ClinicalFeatureEngineering\n",
    "from fe_synthetic_activity_sleep import ActivitySleepFeatureEngineering\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361b046-076e-4548-9361-f5be9605f153",
   "metadata": {},
   "source": [
    "## Clase FeatureEngineeringPipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ad1af-89ff-4772-8662-cbdcfa2c1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineeringPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline maestro para feature engineering multimodal en Alzheimer\n",
    "    \n",
    "    Integra todas las modalidades de datos y genera features finales\n",
    "    incluyendo combinaciones inter-modalidad para score de riesgo compuesto.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Inicializar pipeline de feature engineering\n",
    "        \n",
    "        Args:\n",
    "            config_path: Ruta al archivo de configuraci√≥n (opcional)\n",
    "        \"\"\"\n",
    "        self.setup_logging()\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.feature_processors = {}\n",
    "        self.feature_stats = {}\n",
    "        self.temporal_features = {}\n",
    "        \n",
    "        # Inicializar procesadores por modalidad\n",
    "        self._initialize_processors()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configurar sistema de logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('feature_engineering.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def _load_config(self, config_path: Optional[str]) -> Dict:\n",
    "        \"\"\"Cargar configuraci√≥n del pipeline\"\"\"\n",
    "        default_config = {\n",
    "            'input_path': '../data/processed/integrated/multimodal_alzheimer_dataset.csv',\n",
    "            'metadata_path': '../data/processed/integrated/dataset_metadata.json',\n",
    "            'output_path': '../data/processed/features/',\n",
    "            'temporal_window_days': 365,\n",
    "            'missing_threshold': 0.8,\n",
    "            'correlation_threshold': 0.95,\n",
    "            'feature_selection_methods': ['univariate', 'recursive', 'clinical_relevance'],\n",
    "            'risk_score_components': {\n",
    "                'cognitive': 0.3,\n",
    "                'biomarker': 0.25,\n",
    "                'neuroimaging': 0.2,\n",
    "                'genetic': 0.15,\n",
    "                'activity_sleep': 0.1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if config_path and Path(config_path).exists():\n",
    "            with open(config_path, 'r') as f:\n",
    "                user_config = json.load(f)\n",
    "                default_config.update(user_config)\n",
    "                \n",
    "        return default_config\n",
    "        \n",
    "    def _initialize_processors(self):\n",
    "        \"\"\"Inicializar procesadores espec√≠ficos por modalidad\"\"\"\n",
    "        self.feature_processors = {\n",
    "            'demographics': DemographicsFeatureEngineering(),\n",
    "            'genetics': GeneticsFeatureEngineering(),\n",
    "            'neuroimaging': NeuroimagingFeatureEngineering(),\n",
    "            'biomarkers': BiomarkersFeatureEngineering(),\n",
    "            'clinical': ClinicalFeatureEngineering(),\n",
    "            'activity_sleep': ActivitySleepFeatureEngineering()\n",
    "        }\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cargar dataset integrado\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame con datos multimodales integrados\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üîÑ Cargando dataset integrado...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(self.config['input_path'])\n",
    "            self.logger.info(f\"‚úÖ Dataset cargado: {df.shape[0]} registros, {df.shape[1]} variables\")\n",
    "            \n",
    "            # Cargar metadatos\n",
    "            if Path(self.config['metadata_path']).exists():\n",
    "                with open(self.config['metadata_path'], 'r') as f:\n",
    "                    self.metadata = json.load(f)\n",
    "                    \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Error cargando datos: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def preprocess_temporal_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocesar informaci√≥n temporal para features longitudinales\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con datos originales\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame con features temporales agregadas\n",
    "        \"\"\"\n",
    "        self.logger.info(\"‚è∞ Procesando features temporales...\")\n",
    "        \n",
    "        temporal_cols = ['VISIT_DATE', 'VISDATE', 'DAYS_SINCE_BASELINE']\n",
    "        df_temporal = df.copy()\n",
    "        \n",
    "        # Convertir fechas\n",
    "        for col in temporal_cols:\n",
    "            if col in df_temporal.columns:\n",
    "                df_temporal[col] = pd.to_datetime(df_temporal[col], errors='coerce')\n",
    "                \n",
    "        # Crear features temporales por sujeto\n",
    "        if 'PTID' in df_temporal.columns:\n",
    "            temporal_features = []\n",
    "            \n",
    "            for ptid in df_temporal['PTID'].unique():\n",
    "                subject_data = df_temporal[df_temporal['PTID'] == ptid].copy()\n",
    "                \n",
    "                if len(subject_data) > 1:\n",
    "                    # Ordenar por fecha de visita\n",
    "                    if 'VISIT_DATE' in subject_data.columns:\n",
    "                        subject_data = subject_data.sort_values('VISIT_DATE')\n",
    "                        \n",
    "                        # Features de tendencia temporal\n",
    "                        subject_data['visit_number'] = range(1, len(subject_data) + 1)\n",
    "                        subject_data['days_between_visits'] = subject_data['VISIT_DATE'].diff().dt.days\n",
    "                        subject_data['total_follow_up_days'] = (\n",
    "                            subject_data['VISIT_DATE'].max() - subject_data['VISIT_DATE'].min()\n",
    "                        ).days\n",
    "                        \n",
    "                temporal_features.append(subject_data)\n",
    "                \n",
    "            if temporal_features:\n",
    "                df_temporal = pd.concat(temporal_features, ignore_index=True)\n",
    "                \n",
    "        self.temporal_features = {\n",
    "            'visit_frequency': df_temporal.groupby('PTID').size().to_dict(),\n",
    "            'follow_up_duration': df_temporal.groupby('PTID')['total_follow_up_days'].first().to_dict()\n",
    "        }\n",
    "        \n",
    "        return df_temporal\n",
    "        \n",
    "    def process_features_by_modality(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Procesar features espec√≠ficas por modalidad\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con datos temporales procesados\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame con features procesadas por modalidad\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üî¨ Procesando features por modalidad...\")\n",
    "        \n",
    "        processed_dfs = []\n",
    "        base_df = df[['RID', 'PTID']].drop_duplicates()  # Mantener identificadores\n",
    "        \n",
    "        for modality, processor in self.feature_processors.items():\n",
    "            self.logger.info(f\"   üìä Procesando modalidad: {modality}\")\n",
    "            \n",
    "            try:\n",
    "                # Procesar features espec√≠ficas de la modalidad\n",
    "                modality_features = processor.process_features(df)\n",
    "                \n",
    "                if modality_features is not None and not modality_features.empty:\n",
    "                    # Guardar estad√≠sticas\n",
    "                    self.feature_stats[modality] = {\n",
    "                        'original_features': len([col for col in df.columns if processor.identify_modality_columns(col)]),\n",
    "                        'engineered_features': len(modality_features.columns) - 2,  # Excluyendo RID, PTID\n",
    "                        'missing_percentage': modality_features.isnull().mean().mean() * 100\n",
    "                    }\n",
    "                    \n",
    "                    processed_dfs.append(modality_features)\n",
    "                    self.logger.info(f\"   ‚úÖ {modality}: {len(modality_features.columns)-2} features generadas\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"   ‚ö†Ô∏è {modality}: No se generaron features\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"   ‚ùå Error procesando {modality}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        # Integrar todas las modalidades procesadas\n",
    "        result_df = base_df\n",
    "        for modal_df in processed_dfs:\n",
    "            result_df = result_df.merge(modal_df, on=['RID', 'PTID'], how='left')\n",
    "            \n",
    "        return result_df\n",
    "        \n",
    "    def create_composite_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Crear features compuestas inter-modalidad\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con features por modalidad\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame con features compuestas agregadas\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üîó Creando features compuestas inter-modalidad...\")\n",
    "        \n",
    "        df_composite = df.copy()\n",
    "        \n",
    "        # 1. Ratios cl√≠nicamente relevantes\n",
    "        self._create_biomarker_ratios(df_composite)\n",
    "        \n",
    "        # 2. Scores compuestos por dominio\n",
    "        self._create_domain_scores(df_composite)\n",
    "        \n",
    "        # 3. Features de interacci√≥n gen√©tica-cl√≠nica\n",
    "        self._create_genetic_clinical_interactions(df_composite)\n",
    "        \n",
    "        # 4. √çndices de actividad-sue√±o\n",
    "        self._create_activity_sleep_indices(df_composite)\n",
    "        \n",
    "        # 5. Score de riesgo compuesto final\n",
    "        self._create_composite_risk_score(df_composite)\n",
    "        \n",
    "        return df_composite\n",
    "        \n",
    "    def _create_biomarker_ratios(self, df: pd.DataFrame):\n",
    "        \"\"\"Crear ratios de biomarcadores cl√≠nicamente relevantes\"\"\"\n",
    "        # Ratios t√≠picos en investigaci√≥n de Alzheimer\n",
    "        if 'ABETA_level' in df.columns and 'TAU_level' in df.columns:\n",
    "            df['abeta_tau_ratio'] = df['ABETA_level'] / (df['TAU_level'] + 1e-6)\n",
    "            \n",
    "        # Agregar m√°s ratios seg√∫n biomarcadores disponibles\n",
    "        \n",
    "    def _create_domain_scores(self, df: pd.DataFrame):\n",
    "        \"\"\"Crear scores compuestos por dominio cognitivo\"\"\"\n",
    "        # Score cognitivo compuesto\n",
    "        cognitive_cols = [col for col in df.columns if 'cognitive' in col.lower() or 'mmse' in col.lower()]\n",
    "        if cognitive_cols:\n",
    "            df['composite_cognitive_score'] = df[cognitive_cols].mean(axis=1, skipna=True)\n",
    "            \n",
    "    def _create_genetic_clinical_interactions(self, df: pd.DataFrame):\n",
    "        \"\"\"Crear features de interacci√≥n gen√©tica-cl√≠nica\"\"\"\n",
    "        # Interacciones APOE con otros factores\n",
    "        if 'APOE4_status' in df.columns:\n",
    "            clinical_cols = [col for col in df.columns if 'clinical' in col.lower()]\n",
    "            for col in clinical_cols[:3]:  # Limitar para evitar explosi√≥n de features\n",
    "                if col in df.columns:\n",
    "                    df[f'APOE4_{col}_interaction'] = df['APOE4_status'] * df[col]\n",
    "                    \n",
    "    def _create_activity_sleep_indices(self, df: pd.DataFrame):\n",
    "        \"\"\"Crear √≠ndices compuestos de actividad y sue√±o\"\"\"\n",
    "        # √çndice de calidad de sue√±o\n",
    "        sleep_cols = [col for col in df.columns if 'sleep' in col.lower()]\n",
    "        if sleep_cols:\n",
    "            df['sleep_quality_index'] = df[sleep_cols].mean(axis=1, skipna=True)\n",
    "            \n",
    "        # √çndice de actividad f√≠sica\n",
    "        activity_cols = [col for col in df.columns if 'activity' in col.lower() or 'step' in col.lower()]\n",
    "        if activity_cols:\n",
    "            df['physical_activity_index'] = df[activity_cols].mean(axis=1, skipna=True)\n",
    "            \n",
    "    def _create_composite_risk_score(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Crear score de riesgo compuesto final\n",
    "        \n",
    "        Combina m√∫ltiples modalidades con pesos cl√≠nicamente validados\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üéØ Calculando Score de Riesgo Compuesto...\")\n",
    "        \n",
    "        risk_components = {}\n",
    "        weights = self.config['risk_score_components']\n",
    "        \n",
    "        # Componente cognitivo\n",
    "        if 'composite_cognitive_score' in df.columns:\n",
    "            risk_components['cognitive'] = df['composite_cognitive_score'].fillna(df['composite_cognitive_score'].median())\n",
    "            \n",
    "        # Componente de biomarcadores\n",
    "        biomarker_cols = [col for col in df.columns if 'biomarker' in col.lower() or 'abeta' in col.lower() or 'tau' in col.lower()]\n",
    "        if biomarker_cols:\n",
    "            risk_components['biomarker'] = df[biomarker_cols].mean(axis=1, skipna=True).fillna(0)\n",
    "            \n",
    "        # Componente de neuroimagen\n",
    "        neuroimaging_cols = [col for col in df.columns if 'mri' in col.lower() or 'pet' in col.lower() or 'brain' in col.lower()]\n",
    "        if neuroimaging_cols:\n",
    "            risk_components['neuroimaging'] = df[neuroimaging_cols].mean(axis=1, skipna=True).fillna(0)\n",
    "            \n",
    "        # Componente gen√©tico\n",
    "        genetic_cols = [col for col in df.columns if 'genetic' in col.lower() or 'apoe' in col.lower()]\n",
    "        if genetic_cols:\n",
    "            risk_components['genetic'] = df[genetic_cols].mean(axis=1, skipna=True).fillna(0)\n",
    "            \n",
    "        # Componente actividad-sue√±o\n",
    "        if 'sleep_quality_index' in df.columns and 'physical_activity_index' in df.columns:\n",
    "            risk_components['activity_sleep'] = (df['sleep_quality_index'] + df['physical_activity_index']) / 2\n",
    "            \n",
    "        # Calcular score compuesto\n",
    "        composite_score = pd.Series(0, index=df.index)\n",
    "        \n",
    "        for component, values in risk_components.items():\n",
    "            if component in weights:\n",
    "                # Normalizar componente a escala 0-1\n",
    "                normalized_values = (values - values.min()) / (values.max() - values.min() + 1e-6)\n",
    "                composite_score += weights[component] * normalized_values\n",
    "                \n",
    "        df['composite_risk_score'] = composite_score\n",
    "        df['risk_category'] = pd.cut(composite_score, \n",
    "                                   bins=[0, 0.3, 0.6, 1.0], \n",
    "                                   labels=['Low', 'Medium', 'High'])\n",
    "        \n",
    "        self.logger.info(f\"‚úÖ Score de riesgo calculado - Distribuci√≥n: {df['risk_category'].value_counts().to_dict()}\")\n",
    "        \n",
    "    def feature_selection(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Selecci√≥n de features relevantes\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con todas las features generadas\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame con features seleccionadas\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üéØ Iniciando selecci√≥n de features...\")\n",
    "        \n",
    "        # Remover features con muchos valores faltantes\n",
    "        missing_threshold = self.config['missing_threshold']\n",
    "        missing_rates = df.isnull().mean()\n",
    "        features_to_keep = missing_rates[missing_rates <= missing_threshold].index.tolist()\n",
    "        \n",
    "        df_selected = df[features_to_keep]\n",
    "        self.logger.info(f\"   üìä Features removidas por valores faltantes: {len(df.columns) - len(features_to_keep)}\")\n",
    "        \n",
    "        # Remover features altamente correlacionadas\n",
    "        numeric_cols = df_selected.select_dtypes(include=[np.number]).columns\n",
    "        corr_matrix = df_selected[numeric_cols].corr().abs()\n",
    "        \n",
    "        # Encontrar pares altamente correlacionados\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if corr_matrix.iloc[i, j] >= self.config['correlation_threshold']:\n",
    "                    high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j]))\n",
    "                    \n",
    "        # Remover una feature de cada par correlacionado\n",
    "        features_to_remove = set()\n",
    "        for pair in high_corr_pairs:\n",
    "            # Mantener la feature con menor cantidad de valores faltantes\n",
    "            missing_1 = df_selected[pair[0]].isnull().mean()\n",
    "            missing_2 = df_selected[pair[1]].isnull().mean()\n",
    "            \n",
    "            if missing_1 > missing_2:\n",
    "                features_to_remove.add(pair[0])\n",
    "            else:\n",
    "                features_to_remove.add(pair[1])\n",
    "                \n",
    "        df_selected = df_selected.drop(columns=list(features_to_remove))\n",
    "        self.logger.info(f\"   üîó Features removidas por alta correlaci√≥n: {len(features_to_remove)}\")\n",
    "        \n",
    "        # Selecci√≥n basada en relevancia cl√≠nica (mantener features clave)\n",
    "        clinical_priority_features = [\n",
    "            'composite_risk_score', 'risk_category',\n",
    "            'composite_cognitive_score', 'abeta_tau_ratio',\n",
    "            'sleep_quality_index', 'physical_activity_index'\n",
    "        ]\n",
    "        \n",
    "        priority_features = [col for col in clinical_priority_features if col in df_selected.columns]\n",
    "        other_features = [col for col in df_selected.columns if col not in priority_features]\n",
    "        \n",
    "        # Reorganizar columnas priorizando features cl√≠nicamente relevantes\n",
    "        final_columns = ['RID', 'PTID'] + priority_features + other_features\n",
    "        final_columns = [col for col in final_columns if col in df_selected.columns]\n",
    "        \n",
    "        df_final = df_selected[final_columns]\n",
    "        \n",
    "        self.logger.info(f\"‚úÖ Selecci√≥n completada: {len(df_final.columns)} features finales\")\n",
    "        \n",
    "        return df_final\n",
    "        \n",
    "    def generate_feature_report(self, df_final: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Generar reporte comprehensivo de feature engineering\n",
    "        \n",
    "        Args:\n",
    "            df_final: DataFrame con features finales\n",
    "            \n",
    "        Returns:\n",
    "            Diccionario con estad√≠sticas del proceso\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'dataset_info': {\n",
    "                'total_records': len(df_final),\n",
    "                'total_features': len(df_final.columns),\n",
    "                'unique_subjects': df_final['PTID'].nunique() if 'PTID' in df_final.columns else 'N/A'\n",
    "            },\n",
    "            'modality_stats': self.feature_stats,\n",
    "            'temporal_info': self.temporal_features,\n",
    "            'composite_features': {\n",
    "                'risk_score_distribution': df_final['composite_risk_score'].describe().to_dict() if 'composite_risk_score' in df_final.columns else {},\n",
    "                'risk_categories': df_final['risk_category'].value_counts().to_dict() if 'risk_category' in df_final.columns else {}\n",
    "            },\n",
    "            'data_quality': {\n",
    "                'missing_data_percentage': df_final.isnull().mean().mean() * 100,\n",
    "                'complete_records': len(df_final.dropna()),\n",
    "                'completeness_by_modality': {\n",
    "                    modality: (1 - stats['missing_percentage']/100) * 100 \n",
    "                    for modality, stats in self.feature_stats.items()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    def save_results(self, df_final: pd.DataFrame, report: Dict):\n",
    "        \"\"\"\n",
    "        Guardar resultados del feature engineering\n",
    "        \n",
    "        Args:\n",
    "            df_final: DataFrame con features finales\n",
    "            report: Reporte del proceso\n",
    "        \"\"\"\n",
    "        output_path = Path(self.config['output_path'])\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Guardar dataset final\n",
    "        output_file = output_path / 'multimodal_features_final.csv'\n",
    "        df_final.to_csv(output_file, index=False)\n",
    "        self.logger.info(f\"üíæ Dataset final guardado: {output_file}\")\n",
    "        \n",
    "        # Guardar reporte\n",
    "        report_file = output_path / 'feature_engineering_report.json'\n",
    "        with open(report_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        self.logger.info(f\"üìã Reporte guardado: {report_file}\")\n",
    "        \n",
    "        # Guardar dataset de muestra para an√°lisis\n",
    "        sample_size = min(1000, len(df_final))\n",
    "        sample_df = df_final.sample(n=sample_size, random_state=42)\n",
    "        sample_file = output_path / 'multimodal_features_sample.csv'\n",
    "        sample_df.to_csv(sample_file, index=False)\n",
    "        self.logger.info(f\"üî¨ Muestra guardada: {sample_file}\")\n",
    "        \n",
    "    def run_pipeline(self) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Ejecutar pipeline completo de feature engineering\n",
    "        \n",
    "        Returns:\n",
    "            Tuple con DataFrame final y reporte del proceso\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üöÄ INICIANDO PIPELINE DE FEATURE ENGINEERING\")\n",
    "        self.logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Paso 1: Cargar datos\n",
    "        df = self.load_data()\n",
    "        \n",
    "        # Paso 2: Procesar informaci√≥n temporal\n",
    "        df_temporal = self.preprocess_temporal_data(df)\n",
    "        \n",
    "        # Paso 3: Procesar features por modalidad\n",
    "        df_modality = self.process_features_by_modality(df_temporal)\n",
    "        \n",
    "        # Paso 4: Crear features compuestas\n",
    "        df_composite = self.create_composite_features(df_modality)\n",
    "        \n",
    "        # Paso 5: Selecci√≥n de features\n",
    "        df_final = self.feature_selection(df_composite)\n",
    "        \n",
    "        # Paso 6: Generar reporte\n",
    "        report = self.generate_feature_report(df_final)\n",
    "        \n",
    "        # Paso 7: Guardar resultados\n",
    "        self.save_results(df_final, report)\n",
    "        \n",
    "        self.logger.info(\"=\" * 60)\n",
    "        self.logger.info(\"‚úÖ PIPELINE DE FEATURE ENGINEERING COMPLETADO\")\n",
    "        self.logger.info(f\"üìä Resultado: {len(df_final)} registros √ó {len(df_final.columns)} features\")\n",
    "        \n",
    "        return df_final, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db6b0b-3538-4bbe-adfd-abdca1030b6d",
   "metadata": {},
   "source": [
    "## Funci√≥para Ejecutar el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a1bb5-dc61-4c02-ac7f-7890264e4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Funci√≥n principal para ejecutar el pipeline\"\"\"\n",
    "    try:\n",
    "        # Inicializar y ejecutar pipeline\n",
    "        pipeline = FeatureEngineeringPipeline()\n",
    "        df_final, report = pipeline.run_pipeline()\n",
    "        \n",
    "        print(\"\\nüéØ RESUMEN EJECUTIVO:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìä Registros procesados: {report['dataset_info']['total_records']:,}\")\n",
    "        print(f\"üî¨ Features generadas: {report['dataset_info']['total_features']}\")\n",
    "        print(f\"üë• Sujetos √∫nicos: {report['dataset_info']['unique_subjects']}\")\n",
    "        print(f\"üìà Completitud promedio: {100 - report['data_quality']['missing_data_percentage']:.1f}%\")\n",
    "        \n",
    "        if 'composite_risk_score' in df_final.columns:\n",
    "            print(f\"üéØ Score de riesgo - Media: {df_final['composite_risk_score'].mean():.3f}\")\n",
    "            print(f\"üè∑Ô∏è Categor√≠as de riesgo: {report['composite_features']['risk_categories']}\")\n",
    "            \n",
    "        return df_final, report\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Error en pipeline principal: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed594f3-7738-43aa-a05f-bad4050762d3",
   "metadata": {},
   "source": [
    "## Ejecuci√≥n del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4aa27f-3ab6-47a0-b2bf-cbfc7e28e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_final, report = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340f6a3-bdba-467f-a804-6acb1f508984",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e61b8-1332-4883-bbdd-dbd61d0d8be9",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28646547-bf64-460b-9e01-4654d3953260",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed33bfc-aeaa-40bb-91af-50492becd663",
   "metadata": {},
   "source": [
    "__Abreham Tartalos__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Alzheimer)",
   "language": "python",
   "name": "alzheimer-env-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
